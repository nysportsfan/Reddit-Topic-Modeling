{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Finance Subreddit Capstone Project\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "The goal of this notebook is to extract data containing information about the personal finance subreddit from the pushshift.io Reddit API, select interesting features that can be relevant to the project and clean the dataset so that it is ready for data exploration.\n",
    "\n",
    "- **Import the necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "\n",
    "- **Extract the data from reddit using pushshift.io's API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "sub    = 'personalfinance'     # name of the subreddit you would like to scrape\n",
    "after  = '2018-08-01'    # earliest date that will be scraped\n",
    "before = '2018-09-19'    # latest date that will be scraped\n",
    "fast   = True           # True will be faster, won't pull upvote ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate sqlite\n",
    "sql = sqlite3.connect('personalfinance_.db')\n",
    "cur = sql.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS posts (name TEXT, title TEXT, readable_utc TEXT, permalink TEXT, domain TEXT, url TEXT, author TEXT, score TEXT, upvote_ratio TEXT, num_comments TEXT)')\n",
    "sql.commit()\n",
    "print('Loaded SQL Database and Tables')\n",
    "\n",
    "# Convert the specified dates to strptime\n",
    "after = time.mktime(datetime.datetime.strptime(after, '%Y-%m-%d').timetuple())\n",
    "after = int(after)\n",
    "readable_after = time.strftime('%d %b %Y %I:%M %p', time.localtime(after))\n",
    "before = time.mktime(datetime.datetime.strptime(before, '%Y-%m-%d').timetuple())\n",
    "before = int(before) + 86399\n",
    "readable_before = time.strftime('%d %b %Y %I:%M %p', time.localtime(before))\n",
    "print('Searching for posts between ' + readable_after + ' and ' + readable_before + '.')\n",
    "currentDate = before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pushshift will allow us to retrieve valuable information from reddit submissions including:\n",
    "- Submission ID\n",
    "- Submission title\n",
    "- Submission date\n",
    "- Submission permalink\n",
    "- Submission domain\n",
    "- Submission url\n",
    "- Submission author\n",
    "- Submission score (upvotes)\n",
    "- Submission upvote_ratio (ratio of upvotes to downvotes)\n",
    "- Submission number of comments\n",
    "\n",
    "However, it does not provide us with the flair information.\n",
    "\n",
    "NOTE: This process is extremely computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a new full pull from Pushshift\n",
    "def newpull(thisBefore):\n",
    "    global currentDate\n",
    "    readable_thisBefore = time.strftime('%d %b %Y %I:%M %p', time.localtime(thisBefore))\n",
    "    print('Searching posts before ' + str(readable_thisBefore))\n",
    "    url = 'http://api.pushshift.io/reddit/search/submission/?subreddit=' + sub + '&size=500&before=' + str(thisBefore)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print('    Discussion: HTML Error - ', response.status_code)\n",
    "        time.sleep(60)\n",
    "        return\n",
    "    curJSON = response.json()\n",
    "\n",
    "    # Update each Pushshift result with Reddit data\n",
    "    for child in curJSON['data']:\n",
    "\n",
    "        # Check to see if already added\n",
    "        name = str(child['id'])\n",
    "        cur.execute('SELECT * FROM posts WHERE name == ?', [name])\n",
    "        if cur.fetchone():\n",
    "            print(str(child['id']) + ' skipped (already in database)')\n",
    "            continue\n",
    "\n",
    "        # If not, get more data\n",
    "        if fast is True:\n",
    "            searchURL = 'http://reddit.com/by_id/t3_'\n",
    "        else:\n",
    "            searchURL = 'http://reddit.com/'\n",
    "        url = searchURL + str(name) + '.json'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print('    Discussion: HTML Error - ', response.status_code)\n",
    "            time.sleep(60)\n",
    "            break\n",
    "        postJSON = response.json()\n",
    "        if fast is True:\n",
    "            jsonStart = postJSON\n",
    "        else:\n",
    "            jsonStart = postJSON[0]\n",
    "\n",
    "        # Check to see if Date has passed\n",
    "        global currentDate\n",
    "        created_utc = jsonStart['data']['children'][0]['data']['created_utc']\n",
    "        currentDate = int(created_utc)\n",
    "        if currentDate <= after:\n",
    "            break\n",
    "\n",
    "        # If not, process remaining data\n",
    "        try:\n",
    "            title = str(jsonStart['data']['children'][0]['data']['title'])  # Checks for emojis and other non-printable characters\n",
    "        except UnicodeEncodeError:\n",
    "            title = ''.join(c for c in str(jsonStart['data']['children'][0]['data']['title']) if c in string.printable)\n",
    "        readable_utc = time.strftime('%d %b %Y %I:%M %p', time.localtime(created_utc))\n",
    "        permalink    = (str(jsonStart['data']['children'][0]['data']['permalink']))\n",
    "        domain       = (str(jsonStart['data']['children'][0]['data']['domain']))\n",
    "        url          = (str(jsonStart['data']['children'][0]['data']['url']))\n",
    "        author       = (str(jsonStart['data']['children'][0]['data']['author']))\n",
    "        score        = (str(jsonStart['data']['children'][0]['data']['score']))\n",
    "        num_comments = (str(jsonStart['data']['children'][0]['data']['num_comments']))\n",
    "        if fast is True:\n",
    "            upvote_ratio = 0\n",
    "        else:\n",
    "            upvote_ratio = (str(jsonStart['data']['children'][0]['data']['upvote_ratio']))\n",
    "\n",
    "        # Write it to SQL Database\n",
    "        cur.execute('INSERT INTO posts VALUES(?,?,?,?,?,?,?,?,?,?)', [name, title, readable_utc, permalink, domain, url, author, score, upvote_ratio, num_comments])\n",
    "        sql.commit()\n",
    "\n",
    "# Run the newpull\n",
    "while currentDate >= after:\n",
    "    newpull(currentDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have pulled the data from pushshift, we will need to create a dataframe which will store the relevant information (title, date, time, upvotes, id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to sqlite\n",
    "connection = sqlite3.connect(\"personalfinance_.db\") \n",
    "  \n",
    "# Cursor object \n",
    "crsr = connection.cursor() \n",
    "  \n",
    "# Execute the command to fetch all the data from the table posts \n",
    "crsr.execute(\"SELECT * FROM posts\")  \n",
    "  \n",
    "# Store all the fetched data in the ans variable \n",
    "ans= crsr.fetchall()  \n",
    "\n",
    "# Create empty dataframe\n",
    "columns = ['title']\n",
    "index = range(0,2)\n",
    "df = pd.DataFrame(index = index, columns = columns)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Create new columns and extract the relevant data \n",
    "for n, i in enumerate(ans):\n",
    "    # Create title column\n",
    "    df.loc[n , 'title'] = i[1]\n",
    "    # Create date column\n",
    "    df.loc[n , 'date'] = i[2][:-8]\n",
    "    # Create time column\n",
    "    df.loc[n , 'time'] = i[2][-8:]\n",
    "    # Create upvote column\n",
    "    df.loc[n , 'upvotes'] = i[7]\n",
    "    # Create id column\n",
    "    df.loc[n , 'id'] = i[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Retrieve flair information from Reddit's API**\n",
    "\n",
    "As mentioned before, we still need to extract the flair (which indicates the topic of each submission) from each post. To do this, we will have to initiate a Reddit instance using praw (which gives access to Reddit's API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Reddit instance\n",
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='',\n",
    "                     user_agent='',\n",
    "                    username = '',\n",
    "                    password = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a for-loop that will check and return the appropriate flair for each submission by using its ID as verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in enumerate(df.id):\n",
    "    df.loc[a, 'topic'] = reddit.submission(id = \"{}\".format(b)).link_flair_text\n",
    "    try:\n",
    "        print(a,',', df.loc[a, 'topic'])\n",
    "    except:\n",
    "        print('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Convert Dates into datetime format**\n",
    "\n",
    "Since the data is given as a string, we will need to convert it into datatime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for a,b in enumerate(df.date):\n",
    "     df.loc[a, 'date'] = datetime.strptime(b, '%d %b %Y ').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['date', 'time'], ascending = False).reset_index().drop('index',axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examine basic information**\n",
    "\n",
    "Let's begin by taking a peek at the dataframe's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe: (10183, 6)\n",
      "--------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10183 entries, 0 to 10182\n",
      "Data columns (total 6 columns):\n",
      "title      10183 non-null object\n",
      "date       10183 non-null object\n",
      "time       10183 non-null object\n",
      "upvotes    10183 non-null int64\n",
      "id         10183 non-null object\n",
      "topic      9532 non-null object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 556.9+ KB\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of the dataframe: {}'.format(df.shape))\n",
    "print(20*'-')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column with missing data is 'topic' due to the fact that some posts have been removed and therefore their flairs no longer show up during extraction. To deal with this issue, we can simply fill in 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'].fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt                 1256\n",
       "Other                1205\n",
       "Credit               1048\n",
       "Investing             859\n",
       "Retirement            824\n",
       "Employment            724\n",
       "Housing               709\n",
       "unknown               651\n",
       "Auto                  560\n",
       "Planning              526\n",
       "Saving                524\n",
       "Taxes                 509\n",
       "Budgeting             405\n",
       "Insurance             380\n",
       "Meta                    2\n",
       "THIS IS A SPAMMER       1\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the outlier topics with the topic 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df[(df['topic'] == 'Meta' )| (df['topic'] == 'THIS IS A SPAMMER')]['topic']\n",
    "\n",
    "for id_ in outliers.index:\n",
    "    df.loc[id_,'topic'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(r'C:\\Users\\joshu\\Downloads\\Data\\reddit\\reddit_pf1.csv', engine='python', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "Pre-processing is an important part of machine learning and even more significant for Natural Language Processing tasks because a simple error could result in catastrophic mishaps. \n",
    "\n",
    "There are 3 major components in data pre-processing:\n",
    "\n",
    "**1) Data-cleaning**: We will need to first clean up the text through various steps:\n",
    "\n",
    "- **Lowercase** the words so that the model will not differentiate capitalized words from other words.\n",
    "\n",
    "- **Remove numbers/digits** since the model is interpreting *text* not numbers.\n",
    "\n",
    "- **Remove punctuation** since it is not important for the context.\n",
    "\n",
    "- **Strip white space** since empty strings could be interpreted as text and we want to avoid that.\n",
    "\n",
    "- **Remove stopwords**, which are general words that are very frequent in the English dictionary (ex. because, such, so). Here is a list of some common stopwords: https://www.ranks.nl/stopwords\n",
    "\n",
    "- **Remove noise** that is not picked up through the other cleaning methods. This step can come either before or after tokenization and normalization, or both (ex. dropping words that are less than 2 characters long).\n",
    "\n",
    "**2) Tokenization**: In order to better analyze individual words, we will need to *tokenize* the documents (or in this case, the submission titles) into pieces of words. By doing so, we will be able to use the various NLP libraries to further dissect the tokens.\n",
    "\n",
    "**3) Normalization**: After tokenizing the data, we will need to normalize the text through lemmatization and stemming. Lemmatization is typically a better method since it returns the canonical forms based on a word's lemma. However, this process takes much more time compared to stemming the words, which simply removes the affixes of a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Replace the forward slash with space\n",
    "    text = text.replace('/', ' ')\n",
    "    # Remove all other punctuation without replacement\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Remove digits (excluding strings that contain both digits and letters)\n",
    "    text = ''.join([char for char in text if char not in string.digits])\n",
    "    # Strip whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stopWords])\n",
    "    # Lowercase all words\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           ways make extra side money\n",
       "1    year update legally blind going homeless one i...\n",
       "2                                               kicked\n",
       "3                               online savings account\n",
       "4                      tools managing incomes expenses\n",
       "5    with resources like reddit financial consultin...\n",
       "6    credit hit late payment fee waiver score refle...\n",
       "7                    need help budgetting getting debt\n",
       "8                                                 yo m\n",
       "9        debt collector gave hours pay yelled said ssn\n",
       "Name: clean_title, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_title'] = df['title'].apply(lambda x: preprocess(x))\n",
    "df['clean_title'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row number 2167 has an empty entry\n",
      "Row number 9597 has an empty entry\n"
     ]
    }
   ],
   "source": [
    "# Check to see all clean titles are not empty\n",
    "for num, x in enumerate(df.clean_title):\n",
    "    if not x:\n",
    "        print('Row number {} has an empty entry'.format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>????????? 1 ??? ? ?????? ??? ???????? ? 2018</td>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>11:33 AM</td>\n",
       "      <td>1</td>\n",
       "      <td>9fszwz</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>50/20/30</td>\n",
       "      <td>2018-08-27</td>\n",
       "      <td>11:27 PM</td>\n",
       "      <td>1</td>\n",
       "      <td>9aviyl</td>\n",
       "      <td>Budgeting</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title        date      time  \\\n",
       "2167  ????????? 1 ??? ? ?????? ??? ???????? ? 2018  2018-09-14  11:33 AM   \n",
       "9597                                      50/20/30  2018-08-27  11:27 PM   \n",
       "\n",
       "      upvotes      id      topic clean_title  \n",
       "2167        1  9fszwz        NaN              \n",
       "9597        1  9aviyl  Budgeting              "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what the issue is and correct it\n",
    "df.loc[[2167, 9597]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the titles do not contain much information, let's drop them\n",
    "df.drop([2167, 9597], inplace = True)\n",
    "\n",
    "# Reset the index \n",
    "df = df.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the data as a .csv file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .csv file\n",
    "df.to_csv(r'C:\\Users\\joshu\\Downloads\\Data\\reddit\\reddit_pf1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(r'C:\\Users\\joshu\\Downloads\\Data\\reddit\\reddit_pf1.csv', engine='python', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tokenization**\n",
    "\n",
    "Tokenization is essentially the process of segmenting a text into pieces, such as words, phrases, symbols, etc. \n",
    "\n",
    "Let's create a list of the tokens for each submission title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def create_tokens(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token for token in doc]\n",
    "    return tokens\n",
    "\n",
    "df['tokenized_title'] = df['clean_title'].apply(lambda x: create_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lemmatization**\n",
    "\n",
    "Lemmas are the \"base form\" of a word. \n",
    "\n",
    "Ex. walk, walked, walking, walks would all be derived from the base form 'walk'. \n",
    "\n",
    "Using the tokens that we generated in the column 'tokenized_title', let's next lemmatize the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    # Make sure to remove pronouns (ex. he, she) before returning the lemmas\n",
    "    lemmas = [token.lemma_ for token in text if token.lemma_ not in '-PRON-']\n",
    "    return lemmas\n",
    "\n",
    "df['lemmatized_title'] = df['tokenized_title'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Named Entity Recognition**\n",
    "\n",
    "Named entities are real-world objects that have a name, such as a person, country, or company. spaCy is able to recognize different types of named entities in a document and can return features such as the label (ex. ORG - organization, GPE - geopolitical entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NER(text, label = False):\n",
    "    doc = nlp(text)\n",
    "    if label is False:\n",
    "        NER_list = [(ent.text) for ent in doc.ents]\n",
    "    else:\n",
    "        NER_list = [(ent.label_) for ent in doc.ents]\n",
    "    return NER_list    \n",
    "\n",
    "df['named_entities'] = df['clean_title'].apply(lambda x: create_NER(x))\n",
    "df['entity_labels'] = df['clean_title'].apply(lambda x: create_NER(x, label = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
